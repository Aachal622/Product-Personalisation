# -*- coding: utf-8 -*-
"""Copy of marketing-analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-yAeXVXMNVVFn6VqdnhlhMRJd3bDDAGz

##**Project name: Product Personalisation** 

**Problem Statement :** Predict which offers will be most attractive to each individual customer, resulting in more targeted marketing campaigns and higher brand value.

**Group name: Machine Learning-6**

#**Importing Essential Libraries**
"""

import pandas as pd   # data preprocesing
import  numpy as np   #linear algbra
import matplotlib.pyplot as plt  # data visualization
import seaborn as sns  
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from mlxtend.plotting import plot_confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
import warnings
import plotly.express as ex
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import pickle

"""#**Loading the Data**"""

dataset=pd.read_csv('marketing_data.csv')  #reading the dataset

dataset.head()   #displaying top five rows in the dataset

dataset.shape  #size of the dataset

dataset.columns  #columns for the dataset

dataset.info()

# displaying null values and unique values in the dataset
temp=pd.DataFrame({'null_values': dataset.isnull().sum(),'number_of_unique values' : dataset.nunique()})
temp

dataset.dtypes   #datatype of columns

dataset.drop(columns=['ID','Year_Birth','Dt_Customer'],axis=1,inplace=True)
dataset.head()

"""#####**Describe Function is useful to get information about the categorical as well as the continous variable mainly about the continous variables.**"""

dataset.describe()

# sum of the numbers and divide to number by amount of the numbers
dataset.mean()

#Middle of the point
dataset.median()

#we want to clean the some attributes in columns
dataset.columns = dataset.columns.str.replace(' ', '')

dataset['Income'] = dataset['Income'].str.replace('$', '')
dataset['Income']=dataset['Income'].str.replace(',','').astype('float')

dataset.head(5)

##dived the target with 1 and o
target=dataset[dataset['Response']==1]
non_target=dataset[dataset['Response']==0]

#we use this part for analyzing the data based on response
target_cols=['Response']
cat_cols=dataset.nunique()[dataset.nunique()<6].keys().tolist()
cat_cols=[x for x in cat_cols if x not in target_cols]
num_col=[x for x in dataset.columns if x not in cat_cols+ target_cols]

#the highest part is for graducation and lowest is for basic
dataset['Education'].value_counts()

#Most of then people are married and the Yolo has the lowest portion
dataset['Marital_Status'].value_counts(normalize=True)*100

#we use filter to understand the number of people who are get married and their salary is more than 80000$
dataset[(dataset['Marital_Status']=='Married')& (dataset['Income']>80000)]

dataset[(dataset['Complain']==1)&(dataset['Response']==1)]

dataset['Education'].unique()
Education_MntWines=pd.crosstab(dataset['Education'],dataset['MntWines'])
Education_MntWines

dataset['Response'].unique()
Response_Marital_Status=pd.crosstab(dataset['Response'],dataset['Marital_Status'])
Response_Marital_Status

dataset['Total Spent'] = (dataset['MntFishProducts'] + dataset['MntWines'] + dataset['MntSweetProducts']
                       + dataset['MntFruits'] + dataset['MntMeatProducts'] + dataset['MntGoldProds'] )
dataset['Total Purchase']=(dataset['NumDealsPurchases']+dataset['NumWebPurchases']+dataset['NumStorePurchases']+dataset['NumCatalogPurchases'])

dataset.head(5)

#we want to use the groupby with sum and count with Countries
dataset_grouped_sum=dataset.groupby('Marital_Status',as_index=False)['Income'].agg('sum').rename(columns={'Income':'Income_Sum'})
dataset_grouped_cnt=dataset.groupby('Marital_Status',as_index=False)['Income'].agg('count').rename(columns={'Income':'Income_Count'})

# now we want ro the merge these 2 lines
dataset_grouped_Salary=dataset_grouped_sum.merge(dataset_grouped_cnt,left_on='Marital_Status',right_on='Marital_Status',how='inner')
#now we want to calculate the average salary
dataset_grouped_Salary.loc[:,'Average of Salary']=dataset_grouped_Salary['Income_Sum']/dataset_grouped_Salary['Income_Count']

dataset_grouped_Salary.sort_values('Income_Sum',ascending=False)

dataset_grouped_sum=dataset.groupby('Country',as_index=False)['Income'].agg('sum').rename(columns={'Income':'Income_Sum'})
dataset_grouped_cnt=dataset.groupby('Country',as_index=False)['Income'].agg('count').rename(columns={'Income':'Income_Count'})
#niw we merge these 2 codes
dataset_grouped_Salary=dataset_grouped_sum.merge(dataset_grouped_cnt,left_on='Country',right_on='Country',how='inner')
#calculate the average 
dataset_grouped_Salary.loc[:,'Average of Salary']=dataset_grouped_Salary['Income_Sum']/dataset_grouped_Salary['Income_Count']
dataset_grouped_Salary.sort_values('Income_Sum',ascending=False)

dataset.drop(columns=['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain'])

#Outlier detection using boxplot in Income
num_cols = ['Income']
plt.figure(figsize=(8,6))
dataset[num_cols].boxplot()
plt.title("Outlier Detection in Income", fontsize=18)
plt.show()

#Replacing outliers in Income with the mean Income value
dataset.loc[dataset['Income'] > 130000,'Income']=np.mean(dataset['Income'])

#Outlier Handling using boxplot in Income
num_cols = ['Income']
plt.figure(figsize=(8,6))
dataset[num_cols].boxplot()
plt.title("Outlier Handling in Income", fontsize=18)
plt.show()

"""# **Data Visualization**"""

plt.rcParams['figure.figsize']=(10,5)
dataset['Country'].value_counts().sort_values(ascending=False).plot.bar(color='brown')
plt.xlabel('Country distribution')
plt.ylabel('count')
plt.xticks(rotation=50)
plt.show()

plt.rcParams['figure.figsize']=(30,8)
dataset['Marital_Status'].value_counts().sort_values(ascending=False).plot.pie(y='Marital_Status',autopct="%0.1f%%")
plt.title('distribution of the marriage ')
plt.axis('off')
plt.show() #married has the highest percent with almost 39

fig,ax=plt.subplots(figsize=(10,5))
sns.countplot(dataset['Response'],hue=dataset['Country'],ax=ax)
plt.xlabel('Response')
plt.ylabel('counts')
plt.xticks(rotation=40)
plt.show()

sns.countplot(data=dataset,y='Education',hue='Response',palette='rocket_r')
plt.title('distribute the education with the responses')
plt.figure(figsize=(10,5))
plt.show()

# in the graph below, the total purchase for the response one is more that zero
sns.violinplot(y='Total Purchase',x='Response',data=dataset,color='purple')

# in the graph, as you see, the highest part is for total purchase more than 40 and Income about 160000$
sns.relplot('Total Purchase','Income',data=dataset,color='red',kind='line')

sns.scatterplot('Country','Total Spent',data=dataset,color='purple')
plt.show()# the highest spent is for spain and singapore and the lowest is for Mexico

#the range of the income 
sns.distplot(dataset['Income'], color = 'red')

#the total purchase feom zero to 10 and toal spent from zero to 500 has the highest portion
sns.jointplot('Total Purchase','Total Spent',data=dataset,color='blue',kind='kde')

sns.regplot('Total Spent','Income',data=dataset,color='gold')
plt.xlabel('Total Amount Spent  ')

sns.distplot(x=dataset['Recency'],color='r')

sns.histplot(dataset['Total Purchase'],kde=True,color='silver')

pm = dataset[['Total Purchase', 'Country']].groupby(['Country']).agg([sum])

sns.set_palette('Spectral')
plt.figure(figsize=(10,5))
plt.pie(pm['Total Purchase']['sum'], labels = pm.index, explode = (0, 0.5, 0, 0.3, 1, 0, 0, 0.5),  shadow = True, autopct = '%1.1f%%')
plt.show()

pm=dataset[['Total Spent', 'Marital_Status']].groupby(['Marital_Status']).agg([sum])

sns.set_palette('viridis_r')
plt.figure(figsize=(10,6))
plt.pie(pm['Total Spent']['sum'], labels=pm.index, explode=(0, 0.5, 0, 0.3, 1, 0, 0, 0.5), shadow=True,autopct='%1.1f%%')
plt.show()

plt.figure(figsize = (12, 8))
corr = dataset.corr()
sns.heatmap(corr, linecolor = 'white', linewidths = 1, cmap = 'coolwarm')
plt.show()

"""##**Dealing with the missing values**"""

#missing values from the dataset
dataset.isnull().sum()

"""**Handling the missing values using SimpleImputer**

"""

#handling missing data (Replacing missing data with the mean value)  
imputer= SimpleImputer(missing_values=np.NAN,strategy='mean',fill_value=None, verbose=1, copy=True)
imputer= imputer.fit(dataset[['Income']])
dataset[['Income']]=imputer.transform(dataset[['Income']])

dataset.isnull().sum()

"""###**Segregating the Dependent and Independent variables**"""

#Seperate features from the dataset
features=dataset.drop(["Response","Income","Total Spent","Total Purchase","Recency","Complain","Teenhome"], axis = 1)

features.head()

features.shape

#Seperate labels from the dataset
labels=dataset[['Response']]
labels=np.array(labels,dtype='int64').ravel()

labels

labels.shape

"""###**Encoding Categorical data using OneHotEncoding**"""

# Encoding the Features 
encoder=ColumnTransformer([('encoder',OneHotEncoder(), [0,1,19])],remainder='passthrough')
features=encoder.fit_transform(features)

features

dataset['Marital_Status'].value_counts()

dataset['Education'].value_counts()

dataset['Country'].value_counts()

features[0]

"""**Removing redundancy columns from the features**

"""

features.shape

temp1=features[:, 1:5]

temp1.shape

temp2=features[:, 6:13]

temp2.shape

temp3=features[:, 14:]

temp3.shape

features=np.concatenate((temp1,temp2,temp3), axis=1)

features

features.shape

"""**Splitting the dataset into the Training set and Test set**"""

features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.2, random_state=42)

features_train.shape

features_train

labels_train.shape

labels_test

"""##PCA

Step 1- Standarlization
"""

scaler = StandardScaler()
features_train = scaler.fit_transform(features_train)
features_test = scaler.transform(features_test)

features_train

features_test

"""Step 2 Finding Coviariance matrix. 

step 3: Find out Eigen values and eigen vectors. 

Step 4: Find PCs

"""

pca = PCA(n_components = 0.95)
data_scaled=pca.fit_transform(features_train)

pca.explained_variance_ratio_

plt.rcParams["figure.figsize"] = (12,6)

fig, ax = plt.subplots()
xi = np.arange(1, 29, step=1)
y = np.cumsum(pca.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components')
plt.xticks(np.arange(1, 29, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative variance (%)')
plt.title('The number of components needed to explain variance')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)

ax.grid(axis='x')
plt.show()

pca=PCA(n_components=27)
features_train=pca.fit_transform(features_train)
features_test=pca.transform(features_test)

features_train.shape

features_test.shape

"""#**Fit the data into the model**

###**KNN**
"""

from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier as KNN

"""####Checking Consistency, using Cross Validation

"""

score=cross_val_score(KNN(n_neighbors=3),X=features_train,y=labels_train,cv=10)
score

# Consistency using Mean and standard deviation in percentage
score.mean()*100,score.std()*100

"""###Automating the process of cross validation for different K-Neighbors

"""

def Val_score(n_neighbors):
    '''
    takes range of n_neighbors as input
    returns Mean and Standard Deviation for each value of n_neighbors
    '''
    avg=[]
    standard_d=[]
    for i in n_neighbors:
        
        # 10 fold cross validation for every value of n_neighbors
        score=cross_val_score(KNN(n_neighbors=i),X=features_train,y=labels_train,cv=10)

        #adding mean to avg list
        avg.append(score.mean())

        #adding standard deviation of std list
        standard_d.append(score.std())
    return avg, standard_d

n_neighbors=range(1,50)
mean,std=Val_score(n_neighbors)

"""###Ploting Mean Validation Score for each K value

"""

plt.plot(n_neighbors,mean,color='green',label='mean')
plt.xlabel('n_neighbors')
plt.ylabel('Mean Score')
plt.title('Mean Validation score')

"""###Plotting Standard Deaviation Validation Score for each K value

"""

plt.plot(n_neighbors,std,color='red',label='Standard deviation')
plt.xlabel('n_neighbors')
plt.ylabel('magnitude')
plt.title('Standard Deviation of Validation score')

"""###Trying the optimal model over test set

"""

clf=KNN(n_neighbors=15)
clf.fit(features_train,labels_train)

score1=clf.score(features_train,labels_train)

score1

predknn=clf.predict(features_test)

KNN_cf_matrix = confusion_matrix(labels_test, predknn)
KNN_cf_matrix

plt.figure(figsize=(10,7))
sns.heatmap(KNN_cf_matrix/np.sum(KNN_cf_matrix), annot=True, fmt='.2%', cmap='Reds')

"""#Logisticregression"""

classifier=LogisticRegression()

scores2 = cross_val_score(classifier, features_train, labels_train, scoring='accuracy', cv=10, n_jobs=-1)

scores2

# Consistency using Mean and standard deviation in percentage
scores2.mean()*100,scores2.std()*100

classifier.fit(features_train,labels_train)

predLogistic=classifier.predict(features_test)

predLogistic

labels_test

cf_matrix = confusion_matrix(labels_test, predLogistic)
cf_matrix

plt.figure(figsize=(10,7))
sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')

"""##SVM"""

SVMClassifier=SVC()

scores3 = cross_val_score(SVMClassifier, features_train, labels_train, scoring='accuracy', cv=10, n_jobs=-1)

scores3

# Consistency using Mean and standard deviation in percentage
scores3.mean()*100,scores3.std()*100

SVMClassifier.fit(features_train,labels_train)

predSVM=SVMClassifier.predict(features_test)

predSVM

labels_test

SVC_cf_matrix = confusion_matrix(labels_test, predSVM)
SVC_cf_matrix

plt.figure(figsize=(10,7))
sns.heatmap(SVC_cf_matrix/np.sum(SVC_cf_matrix), annot=True, fmt='.2%', cmap='binary')

acc_score=[]
pre_score=[]
re_score=[]
fscore=[]
l1=[predknn,predLogistic,predSVM]
for i in l1:
  acc_score.append(accuracy_score(labels_test, i))
  pre_score.append(precision_score(labels_test, i))
  re_score.append(recall_score(labels_test, i))
  fscore.append(f1_score(labels_test, i))

df=pd.DataFrame()
df['model']=['KNN', 'LogisticRegressior', 'SupportVectorClassifier']
df['accuracy_score']=acc_score
df['precision_score']=pre_score
df['recall_score']=re_score
df['f1_score']=fscore

df

file='user_offer_attraction.pkl'
pickle.dump(clf,open(file,'wb'))

model_clf=pickle.load(open('user_offer_attraction.pkl','rb'))

pred=model_clf.predict(features_test)

pred

labels_test

"""# Result
* As you see, we use different visualization part with different attributes and also we divided the dta to num columns and cat columns with target and non target marketting and at the end of the part, we train and test the model with 7 different algorithms and we can see the percentage which one has the best one .
"""